{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_46CazV3XSCD"
   },
   "source": [
    "Define model training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "fNYQywdWXeLM"
   },
   "outputs": [],
   "source": [
    "LR = 5e-4\n",
    "EPOCHS = 5\n",
    "EPOCHS_PER_UPDATE = 1\n",
    "RUNNAME = \"Sen1Floods11\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9FJmTnZXjxj"
   },
   "source": [
    "Define functions to process and augment training and testing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "mBkfav0Eajqg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as F\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "class InMemoryDataset(torch.utils.data.Dataset):\n",
    "  \n",
    "  def __init__(self, data_list, preprocess_func):\n",
    "    self.data_list = data_list\n",
    "    self.preprocess_func = preprocess_func\n",
    "  \n",
    "  def __getitem__(self, i):\n",
    "    return self.preprocess_func(self.data_list[i])\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.data_list)\n",
    "\n",
    "\n",
    "def processAndAugment(data):\n",
    "  (x,y) = data\n",
    "  im,label = x.copy(), y.copy()\n",
    "\n",
    "  # convert to PIL for easier transforms\n",
    "  im1 = Image.fromarray(im[0])\n",
    "  im2 = Image.fromarray(im[1])\n",
    "  label = Image.fromarray(label.squeeze())\n",
    "\n",
    "  # Get params for random transforms\n",
    "  i, j, h, w = transforms.RandomCrop.get_params(im1, (256, 256))\n",
    "  \n",
    "  im1 = F.crop(im1, i, j, h, w)\n",
    "  im2 = F.crop(im2, i, j, h, w)\n",
    "  label = F.crop(label, i, j, h, w)\n",
    "  if random.random() > 0.5:\n",
    "    im1 = F.hflip(im1)\n",
    "    im2 = F.hflip(im2)\n",
    "    label = F.hflip(label)\n",
    "  if random.random() > 0.5:\n",
    "    im1 = F.vflip(im1)\n",
    "    im2 = F.vflip(im2)\n",
    "    label = F.vflip(label)\n",
    "  \n",
    "  norm = transforms.Normalize([0.6851, 0.5235], [0.0820, 0.1102])\n",
    "  im = torch.stack([transforms.ToTensor()(im1).squeeze(), transforms.ToTensor()(im2).squeeze()])\n",
    "  im = norm(im)\n",
    "  label = transforms.ToTensor()(label).squeeze()\n",
    "  if torch.sum(label.gt(.003) * label.lt(.004)):\n",
    "    label *= 255\n",
    "  label = label.round()\n",
    "\n",
    "  return im, label\n",
    "\n",
    "\n",
    "def processTestIm(data):\n",
    "  (x,y) = data\n",
    "  im,label = x.copy(), y.copy()\n",
    "  norm = transforms.Normalize([0.6851, 0.5235], [0.0820, 0.1102])\n",
    "\n",
    "  # convert to PIL for easier transforms\n",
    "  im_c1 = Image.fromarray(im[0]).resize((512,512))\n",
    "  im_c2 = Image.fromarray(im[1]).resize((512,512))\n",
    "  label = Image.fromarray(label.squeeze()).resize((512,512))\n",
    "\n",
    "  im_c1s = [F.crop(im_c1, 0, 0, 256, 256), F.crop(im_c1, 0, 256, 256, 256),\n",
    "            F.crop(im_c1, 256, 0, 256, 256), F.crop(im_c1, 256, 256, 256, 256)]\n",
    "  im_c2s = [F.crop(im_c2, 0, 0, 256, 256), F.crop(im_c2, 0, 256, 256, 256),\n",
    "            F.crop(im_c2, 256, 0, 256, 256), F.crop(im_c2, 256, 256, 256, 256)]\n",
    "  labels = [F.crop(label, 0, 0, 256, 256), F.crop(label, 0, 256, 256, 256),\n",
    "            F.crop(label, 256, 0, 256, 256), F.crop(label, 256, 256, 256, 256)]\n",
    "\n",
    "  ims = [torch.stack((transforms.ToTensor()(x).squeeze(),\n",
    "                    transforms.ToTensor()(y).squeeze()))\n",
    "                    for (x,y) in zip(im_c1s, im_c2s)]\n",
    "  \n",
    "  ims = [norm(im) for im in ims]\n",
    "  ims = torch.stack(ims)\n",
    "  \n",
    "  labels = [(transforms.ToTensor()(label).squeeze()) for label in labels]\n",
    "  labels = torch.stack(labels)\n",
    "  \n",
    "  if torch.sum(labels.gt(.003) * labels.lt(.004)):\n",
    "    labels *= 255\n",
    "  labels = labels.round()\n",
    "  \n",
    "  return ims, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzmZIRuoeAuJ"
   },
   "source": [
    "Load *flood water* train, test, and validation data from splits. In this example, this is the data we will use to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "rQUnYCIBeG21"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import rasterio\n",
    "\n",
    "def getArrFlood(fname):\n",
    "  return rasterio.open(fname).read()\n",
    "\n",
    "def download_flood_water_data_from_list(l):\n",
    "  i = 0\n",
    "  tot_nan = 0\n",
    "  tot_good = 0\n",
    "  flood_data = []\n",
    "  for (im_fname, mask_fname) in l:\n",
    "    # Updated path to use the correct HandLabeled directory structure\n",
    "    s1_path = os.path.join(\"D:/Flood-Segmentation/dataset/HandLabeled/\", im_fname)\n",
    "    label_path = os.path.join(\"D:/Flood-Segmentation/dataset/HandLabeled/\", mask_fname)\n",
    "    \n",
    "    if not os.path.exists(s1_path):\n",
    "      print(f\"S1 file not found: {s1_path}\")\n",
    "      continue\n",
    "    if not os.path.exists(label_path):\n",
    "      print(f\"Label file not found: {label_path}\")\n",
    "      continue\n",
    "      \n",
    "    arr_x = np.nan_to_num(getArrFlood(s1_path))\n",
    "    arr_y = getArrFlood(label_path)\n",
    "    arr_y[arr_y == -1] = 255 \n",
    "    \n",
    "    arr_x = np.clip(arr_x, -50, 1)\n",
    "    arr_x = (arr_x + 50) / 51\n",
    "      \n",
    "    if i % 100 == 0:\n",
    "      print(im_fname, mask_fname)\n",
    "    i += 1\n",
    "    flood_data.append((arr_x,arr_y))\n",
    "\n",
    "  return flood_data\n",
    "\n",
    "def load_flood_train_data(input_root, label_root):\n",
    "  fname = \"D:/Flood-Segmentation/dataset/flood_handlabeled/flood_train_data.csv\"\n",
    "  training_files = []\n",
    "  with open(fname) as f:\n",
    "    for line in csv.reader(f):\n",
    "      # The CSV already contains the correct filenames, just prepend the subdirectory\n",
    "      training_files.append(tuple((input_root+line[0], label_root+line[1])))\n",
    "\n",
    "  return download_flood_water_data_from_list(training_files)\n",
    "\n",
    "def load_flood_valid_data(input_root, label_root):\n",
    "  fname = \"D:/Flood-Segmentation/dataset/flood_handlabeled/flood_valid_data.csv\"\n",
    "  validation_files = []\n",
    "  with open(fname) as f:\n",
    "    for line in csv.reader(f):\n",
    "      validation_files.append(tuple((input_root+line[0], label_root+line[1])))\n",
    "\n",
    "  return download_flood_water_data_from_list(validation_files)\n",
    "\n",
    "def load_flood_test_data(input_root, label_root):\n",
    "  fname = \"D:/Flood-Segmentation/dataset/flood_handlabeled/flood_test_data.csv\"\n",
    "  testing_files = []\n",
    "  with open(fname) as f:\n",
    "    for line in csv.reader(f):\n",
    "      testing_files.append(tuple((input_root+line[0], label_root+line[1])))\n",
    "  \n",
    "  return download_flood_water_data_from_list(testing_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFp9jrHYfOUh"
   },
   "source": [
    "Load training data and validation data. Note that here, we have chosen to train and validate our model on flood data. However, you can simply replace the load function call with one of the options defined above to load a different dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "ZcqPlsjBffXx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1Hand/Ghana_103272_S1Hand.tif LabelHand/Ghana_103272_LabelHand.tif\n",
      "S1Hand/Pakistan_132143_S1Hand.tif LabelHand/Pakistan_132143_LabelHand.tif\n",
      "S1Hand/Sri-Lanka_916628_S1Hand.tif LabelHand/Sri-Lanka_916628_LabelHand.tif\n",
      "S1Hand/Ghana_5079_S1Hand.tif LabelHand/Ghana_5079_LabelHand.tif\n"
     ]
    }
   ],
   "source": [
    "train_data = load_flood_train_data('S1Hand/', 'LabelHand/')\n",
    "train_dataset = InMemoryDataset(train_data, processAndAugment)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True, sampler=None,\n",
    "                  batch_sampler=None, num_workers=0, collate_fn=None,\n",
    "                  pin_memory=True, drop_last=False, timeout=0,\n",
    "                  worker_init_fn=None)\n",
    "train_iter = iter(train_loader)\n",
    "\n",
    "valid_data = load_flood_valid_data('S1Hand/', 'LabelHand/') \n",
    "valid_dataset = InMemoryDataset(valid_data, processTestIm)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=4, shuffle=True, sampler=None,\n",
    "                  batch_sampler=None, num_workers=0, collate_fn=lambda x: (torch.cat([a[0] for a in x], 0), torch.cat([a[1] for a in x], 0)),\n",
    "                  pin_memory=True, drop_last=False, timeout=0,\n",
    "                  worker_init_fn=None)\n",
    "valid_iter = iter(valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3aAhUi2fp7M"
   },
   "source": [
    "Define the network. For our purposes, we use ResNet50. However, if you wish to test a different model framework, optimizer, or loss function you can simply replace those here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "5cp4uXI1f9dr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RAJ\\miniconda3\\envs\\tf\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\RAJ\\miniconda3\\envs\\tf\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\RAJ\\miniconda3\\envs\\tf\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained_backbone' is deprecated since 0.13 and may be removed in the future, please use 'weights_backbone' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\RAJ\\miniconda3\\envs\\tf\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights_backbone' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights_backbone=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "net = models.segmentation.fcn_resnet50(pretrained=False, num_classes=2, pretrained_backbone=False)\n",
    "net.backbone.conv1 = nn.Conv2d(2, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor([1,8]).float().cuda(), ignore_index=255) \n",
    "optimizer = torch.optim.AdamW(net.parameters(),lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, len(train_loader) * 10, T_mult=2, eta_min=0, last_epoch=-1)\n",
    "\n",
    "def convertBNtoGN(module, num_groups=16):\n",
    "  if isinstance(module, torch.nn.modules.batchnorm.BatchNorm2d):\n",
    "    return nn.GroupNorm(num_groups, module.num_features,\n",
    "                        eps=module.eps, affine=module.affine)\n",
    "    if module.affine:\n",
    "        mod.weight.data = module.weight.data.clone().detach()\n",
    "        mod.bias.data = module.bias.data.clone().detach()\n",
    "\n",
    "  for name, child in module.named_children():\n",
    "      module.add_module(name, convertBNtoGN(child, num_groups=num_groups))\n",
    "\n",
    "  return module\n",
    "\n",
    "net = convertBNtoGN(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_Sy3ALGgQjf"
   },
   "source": [
    "Define assessment metrics. For our purposes, we use overall accuracy and mean intersection over union. However, we also include functions for calculating true positives, false positives, true negatives, and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "bwxC-fVBgUIb"
   },
   "outputs": [],
   "source": [
    "def computeIOU(output, target):\n",
    "  output = torch.argmax(output, dim=1).flatten() \n",
    "  target = target.flatten()\n",
    "  \n",
    "  no_ignore = target.ne(255).cuda()\n",
    "  output = output.masked_select(no_ignore)\n",
    "  target = target.masked_select(no_ignore)\n",
    "  intersection = torch.sum(output * target)\n",
    "  union = torch.sum(target) + torch.sum(output) - intersection\n",
    "  iou = (intersection + .0000001) / (union + .0000001)\n",
    "  \n",
    "  if iou != iou:\n",
    "    print(\"failed, replacing with 0\")\n",
    "    iou = torch.tensor(0).float()\n",
    "  \n",
    "  return iou\n",
    "  \n",
    "def computeAccuracy(output, target):\n",
    "  output = torch.argmax(output, dim=1).flatten() \n",
    "  target = target.flatten()\n",
    "  \n",
    "  no_ignore = target.ne(255).cuda()\n",
    "  output = output.masked_select(no_ignore)\n",
    "  target = target.masked_select(no_ignore)\n",
    "  correct = torch.sum(output.eq(target))\n",
    "  \n",
    "  return correct.float() / len(target)\n",
    "\n",
    "def truePositives(output, target):\n",
    "  output = torch.argmax(output, dim=1).flatten() \n",
    "  target = target.flatten()\n",
    "  no_ignore = target.ne(255).cuda()\n",
    "  output = output.masked_select(no_ignore)\n",
    "  target = target.masked_select(no_ignore)\n",
    "  correct = torch.sum(output * target)\n",
    "  \n",
    "  return correct\n",
    "\n",
    "def trueNegatives(output, target):\n",
    "  output = torch.argmax(output, dim=1).flatten() \n",
    "  target = target.flatten()\n",
    "  no_ignore = target.ne(255).cuda()\n",
    "  output = output.masked_select(no_ignore)\n",
    "  target = target.masked_select(no_ignore)\n",
    "  output = (output == 0)\n",
    "  target = (target == 0)\n",
    "  correct = torch.sum(output * target)\n",
    "  \n",
    "  return correct\n",
    "\n",
    "def falsePositives(output, target):\n",
    "  output = torch.argmax(output, dim=1).flatten() \n",
    "  target = target.flatten()\n",
    "  no_ignore = target.ne(255).cuda()\n",
    "  output = output.masked_select(no_ignore)\n",
    "  target = target.masked_select(no_ignore)\n",
    "  output = (output == 1)\n",
    "  target = (target == 0)\n",
    "  correct = torch.sum(output * target)\n",
    "  \n",
    "  return correct\n",
    "\n",
    "def falseNegatives(output, target):\n",
    "  output = torch.argmax(output, dim=1).flatten() \n",
    "  target = target.flatten()\n",
    "  no_ignore = target.ne(255).cuda()\n",
    "  output = output.masked_select(no_ignore)\n",
    "  target = target.masked_select(no_ignore)\n",
    "  output = (output == 0)\n",
    "  target = (target == 1)\n",
    "  correct = torch.sum(output * target)\n",
    "  \n",
    "  return correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lun5tGoYgjWX"
   },
   "source": [
    "Define training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "DubsYZ8GgkxD"
   },
   "outputs": [],
   "source": [
    "training_losses = []\n",
    "training_accuracies = []\n",
    "training_ious = []\n",
    "\n",
    "def train_loop(inputs, labels, net, optimizer, scheduler):\n",
    "  global running_loss\n",
    "  global running_iou\n",
    "  global running_count\n",
    "  global running_accuracy\n",
    "  \n",
    "  # zero the parameter gradients\n",
    "  optimizer.zero_grad()\n",
    "  net = net.cuda()\n",
    "  \n",
    "  # forward + backward + optimize\n",
    "  outputs = net(inputs.cuda())\n",
    "  loss = criterion(outputs[\"out\"], labels.long().cuda())\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  scheduler.step()\n",
    "\n",
    "  running_loss += loss\n",
    "  running_iou += computeIOU(outputs[\"out\"], labels.cuda())\n",
    "  running_accuracy += computeAccuracy(outputs[\"out\"], labels.cuda())\n",
    "  running_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iM3Jz__hgshh"
   },
   "source": [
    "Define validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "_GmVaoRvguic"
   },
   "outputs": [],
   "source": [
    "valid_losses = []\n",
    "valid_accuracies = []\n",
    "valid_ious = []\n",
    "\n",
    "def validation_loop(validation_data_loader, net):\n",
    "  global running_loss\n",
    "  global running_iou\n",
    "  global running_count\n",
    "  global running_accuracy\n",
    "  global max_valid_iou\n",
    "\n",
    "  global training_losses\n",
    "  global training_accuracies\n",
    "  global training_ious\n",
    "  global valid_losses\n",
    "  global valid_accuracies\n",
    "  global valid_ious\n",
    "\n",
    "  net = net.eval()\n",
    "  net = net.cuda()\n",
    "  count = 0\n",
    "  iou = 0\n",
    "  loss = 0\n",
    "  accuracy = 0\n",
    "  with torch.no_grad():\n",
    "      for (images, labels) in validation_data_loader:\n",
    "          net = net.cuda()\n",
    "          outputs = net(images.cuda())\n",
    "          valid_loss = criterion(outputs[\"out\"], labels.long().cuda())\n",
    "          valid_iou = computeIOU(outputs[\"out\"], labels.cuda())\n",
    "          valid_accuracy = computeAccuracy(outputs[\"out\"], labels.cuda())\n",
    "          iou += valid_iou\n",
    "          loss += valid_loss\n",
    "          accuracy += valid_accuracy\n",
    "          count += 1\n",
    "\n",
    "  iou = iou / count\n",
    "  accuracy = accuracy / count\n",
    "\n",
    "  if iou > max_valid_iou:\n",
    "    max_valid_iou = iou\n",
    "    save_path = os.path.join(\"checkpoints\", \"{}_{}_{}.cp\".format(RUNNAME, i, iou.item()))\n",
    "    torch.save(net.state_dict(), save_path)\n",
    "    print(\"model saved at\", save_path)\n",
    "\n",
    "  loss = loss / count\n",
    "  print(\"Training Loss:\", running_loss / running_count)\n",
    "  print(\"Training IOU:\", running_iou / running_count)\n",
    "  print(\"Training Accuracy:\", running_accuracy / running_count)\n",
    "  print(\"Validation Loss:\", loss)\n",
    "  print(\"Validation IOU:\", iou)\n",
    "  print(\"Validation Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "  training_losses.append(running_loss / running_count)\n",
    "  training_accuracies.append(running_accuracy / running_count)\n",
    "  training_ious.append(running_iou / running_count)\n",
    "  valid_losses.append(loss)\n",
    "  valid_accuracies.append(accuracy)\n",
    "  valid_ious.append(iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBMattYshiUj"
   },
   "source": [
    "Define testing loop (here, you can replace assessment metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "mI_mhL_ehjot"
   },
   "outputs": [],
   "source": [
    "def test_loop(test_data_loader, net):\n",
    "  net = net.eval()\n",
    "  net = net.cuda()\n",
    "  count = 0\n",
    "  iou = 0\n",
    "  loss = 0\n",
    "  accuracy = 0\n",
    "  with torch.no_grad():\n",
    "      for (images, labels) in tqdm(test_data_loader):\n",
    "          net = net.cuda()\n",
    "          outputs = net(images.cuda())\n",
    "          valid_loss = criterion(outputs[\"out\"], labels.long().cuda())\n",
    "          valid_iou = computeIOU(outputs[\"out\"], labels.cuda())\n",
    "          iou += valid_iou\n",
    "          accuracy += computeAccuracy(outputs[\"out\"], labels.cuda())\n",
    "          count += 1\n",
    "\n",
    "  iou = iou / count\n",
    "  print(\"Test IOU:\", iou)\n",
    "  print(\"Test Accuracy:\", accuracy / count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cy9Fii06h17Q"
   },
   "source": [
    "Define training and validation scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "NZuKVC6wh4Go"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "running_loss = 0\n",
    "running_iou = 0\n",
    "running_count = 0\n",
    "running_accuracy = 0\n",
    "\n",
    "training_losses = []\n",
    "training_accuracies = []\n",
    "training_ious = []\n",
    "valid_losses = []\n",
    "valid_accuracies = []\n",
    "valid_ious = []\n",
    "\n",
    "\n",
    "def train_epoch(net, optimizer, scheduler, train_iter):\n",
    "  for (inputs, labels) in tqdm(train_iter):\n",
    "    train_loop(inputs.cuda(), labels.cuda(), net.cuda(), optimizer, scheduler)\n",
    " \n",
    "\n",
    "def train_validation_loop(net, optimizer, scheduler, train_loader,\n",
    "                          valid_loader, num_epochs, cur_epoch):\n",
    "  global running_loss\n",
    "  global running_iou\n",
    "  global running_count\n",
    "  global running_accuracy\n",
    "  net = net.train()\n",
    "  running_loss = 0\n",
    "  running_iou = 0\n",
    "  running_count = 0\n",
    "  running_accuracy = 0\n",
    "  \n",
    "  for i in tqdm(range(num_epochs)):\n",
    "    train_iter = iter(train_loader)\n",
    "    train_epoch(net, optimizer, scheduler, train_iter)\n",
    "  clear_output()\n",
    "  \n",
    "  print(\"Current Epoch:\", cur_epoch)\n",
    "  validation_loop(iter(valid_loader), net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3I88aY5iAWD"
   },
   "source": [
    "Train model and assess metrics over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "8MRpxUGWiDTu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Epoch: 0\n",
      "model saved at checkpoints\\Sen1Floods11_0_0.362542986869812.cp\n",
      "Training Loss: tensor(0.5640, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training IOU: tensor(0.2611, device='cuda:0')\n",
      "Training Accuracy: tensor(0.8127, device='cuda:0')\n",
      "Validation Loss: tensor(0.3458, device='cuda:0')\n",
      "Validation IOU: tensor(0.3625, device='cuda:0')\n",
      "Validation Accuracy: tensor(0.8871, device='cuda:0')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m epochs\u001b[38;5;241m.\u001b[39mappend(i)\n\u001b[0;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m epochs\n\u001b[1;32m---> 20\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining losses\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(x, training_accuracies, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtab:orange\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(x, training_ious, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtab:purple\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining iou\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\RAJ\\miniconda3\\envs\\tf\\lib\\site-packages\\matplotlib\\pyplot.py:3590\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3582\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[0;32m   3583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mplot\u001b[39m(\n\u001b[0;32m   3584\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3588\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3589\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[1;32m-> 3590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gca()\u001b[38;5;241m.\u001b[39mplot(\n\u001b[0;32m   3591\u001b[0m         \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m   3592\u001b[0m         scalex\u001b[38;5;241m=\u001b[39mscalex,\n\u001b[0;32m   3593\u001b[0m         scaley\u001b[38;5;241m=\u001b[39mscaley,\n\u001b[0;32m   3594\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data} \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[0;32m   3595\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3596\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\RAJ\\miniconda3\\envs\\tf\\lib\\site-packages\\matplotlib\\axes\\_axes.py:1724\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1481\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1482\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1483\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1721\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1722\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1723\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[1;32m-> 1724\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1725\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m   1726\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32mc:\\Users\\RAJ\\miniconda3\\envs\\tf\\lib\\site-packages\\matplotlib\\axes\\_base.py:303\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    302\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 303\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\RAJ\\miniconda3\\envs\\tf\\lib\\site-packages\\matplotlib\\axes\\_base.py:489\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(xy) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    488\u001b[0m     x \u001b[38;5;241m=\u001b[39m _check_1d(xy[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m--> 489\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43m_check_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxy\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    491\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m index_of(xy[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\RAJ\\miniconda3\\envs\\tf\\lib\\site-packages\\matplotlib\\cbook.py:1358\u001b[0m, in \u001b[0;36m_check_1d\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1352\u001b[0m \u001b[38;5;66;03m# plot requires `shape` and `ndim`.  If passed an\u001b[39;00m\n\u001b[0;32m   1353\u001b[0m \u001b[38;5;66;03m# object that doesn't provide them, then force to numpy array.\u001b[39;00m\n\u001b[0;32m   1354\u001b[0m \u001b[38;5;66;03m# Note this will strip unit information.\u001b[39;00m\n\u001b[0;32m   1355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1356\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1357\u001b[0m         \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m-> 1358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matleast_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1359\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\RAJ\\miniconda3\\envs\\tf\\lib\\site-packages\\numpy\\core\\shape_base.py:65\u001b[0m, in \u001b[0;36matleast_1d\u001b[1;34m(*arys)\u001b[0m\n\u001b[0;32m     63\u001b[0m res \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ary \u001b[38;5;129;01min\u001b[39;00m arys:\n\u001b[1;32m---> 65\u001b[0m     ary \u001b[38;5;241m=\u001b[39m \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ary\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     67\u001b[0m         result \u001b[38;5;241m=\u001b[39m ary\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\RAJ\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\_tensor.py:1225\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   1224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "max_valid_iou = 0\n",
    "start = 0\n",
    "\n",
    "epochs = []\n",
    "training_losses = []\n",
    "training_accuracies = []\n",
    "training_ious = []\n",
    "valid_losses = []\n",
    "valid_accuracies = []\n",
    "valid_ious = []\n",
    "\n",
    "for i in range(start, 1000):\n",
    "  train_validation_loop(net, optimizer, scheduler, train_loader, valid_loader, 10, i)\n",
    "  epochs.append(i)\n",
    "  x = epochs\n",
    "  plt.plot(x, training_losses, label='training losses')\n",
    "  plt.plot(x, training_accuracies, 'tab:orange', label='training accuracy')\n",
    "  plt.plot(x, training_ious, 'tab:purple', label='training iou')\n",
    "  plt.plot(x, valid_losses, label='valid losses')\n",
    "  plt.plot(x, valid_accuracies, 'tab:red',label='valid accuracy')\n",
    "  plt.plot(x, valid_ious, 'tab:green',label='valid iou')\n",
    "  plt.legend(loc=\"upper left\")\n",
    "\n",
    "  display(plt.show())\n",
    "\n",
    "  print(\"max valid iou:\", max_valid_iou)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
